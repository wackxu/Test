Update the way in which kubelet determines whether a container meets the given spec
@lichuqiang

Background:
Currently kubelet hashes the container spec and store it with the container so that it can detect whether the container meets the current spec. If the hashes do not match, kubelet would kill the container and create a new one. The most notable side effect of this mechanism is that during upgrade, if a new field is added to the container spec in the 1.x kubernetes version, and kubelet is upgraded to 1.x from 1.x-1 version, all containers created by the 1.x-1 version kubelet will be killed, which users would not like to see.
Now that latest docker supports live-restore, we should consider fixing this in k8s.
Alternatives considered
Several alternatives have been considered:
Save serialized container spec instead of hash label
as @liggitt suggested, Instead of store hash values in container labels, we can save the serialized container spec we had when we launched the container. And during detection, kubelet can load it, apply defaults, and do a semantic deepequal to see if there was an actual diff that required restarting.

One concern is that the serialized content can be of great length, and there is a risk of exceeding the runtime label length limit.
Hash certain fields instead of whole container spec
Instead of hash the whole container spec, we can maintain a list of container fields of different releases that should be taken into consideration during hash, and identify the hash values with release prefix.
A field list may looks like:

release1: {field1, field2...fieldN}
release2: {field1, field2...fieldN, fieldN+1}

and imagine the kubelet has been upgraded to release2 (new container field fieldN+1 added), when walking through the pods, if it found a container has a hash with a prefix of "release1", then it knows the container should be hashed with the fields of release1, thus, the result hash would not change.

A downside is that we might need to maintain the field list manually (e.g deprecate the field of old releases).
And also, if a user changes newly added container fields of a static pod that is launched in old way, he will not see the pods restart as expect, instead, he’ll have to delete and recreate it.

Add release labels to container fields
Instead of hardcode the fields that need hash in a list, we can also consider to add labels in container fields API, to imply that the fields should be included in hash of certain release.
For example:

type Container struct {
    foo string `containerSerializedSpecLabel:"since v1.9"`
    bar string `containerSerializedSpecLabel:"since v1.10"`
}

Then, “foo” will be included for checking container hash like "v1.9-xxxxxxxx"
“foo” and “bar” will be included for checking container hash like "v1.10-xxxxxxxx"

But the approach requires changes on API mechanism, which lots of people may not want to see.
Backwards compatible
For all of the approaches above, for the first time a kubelet is upgraded to a version with the change introduced, containers launched in old ways will be restart, because all of the three approaches change the way we save/apply container identity.

If we do not expect to see this, we’ll need extra mechanism to ensure backwards compatible.
For example, we can keep old container labels as is, and add new labels to apply newly calculated identity values. When kubelet finds a container is labeled in the old way, it’ll skip it. Downside is that we’ll have to recreate pods manually if we want to update a static pod that is labeled in the old way.

For the 2nd and 3rd approach, we can also consider mark all of existing fields in container as in need of hash (instead of only those mutable fields), thus, hash values would not change upon upgrades.
